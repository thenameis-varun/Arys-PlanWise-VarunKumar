## ðŸ”¹ Overview

This project explores building an **AI-powered local application** using multiple open-source LLMs and frameworks. The goal is to run models entirely **offline** (no API calls to OpenAI/Anthropic), while integrating them into a **Streamlit-based interface** for interactive usage.

Currently, we are experimenting with different model backends (`GPT4All` and `llama-cpp-python`) to identify the most reliable and performant stack for inference.

---

## ðŸ”¹ Tech Stack

### Backend / AI Inference

* **[GPT4All](https://github.com/nomic-ai/gpt4all)**
  Python bindings and desktop app for running `.gguf` quantized LLMs locally.

  * Installed via `pip install gpt4all`
  * Initially used to attempt inference with DeepSeek models.

* **[llama-cpp-python](https://github.com/abetlen/llama-cpp-python)**
  Python bindings for [llama.cpp](https://github.com/ggerganov/llama.cpp), a fast C++ inference engine supporting `.gguf` models.

  * Installed via `pip install llama-cpp-python`
  * Currently used for running **DeepSeek-R1-Distill-Qwen-1.5B-Q4\_0.gguf**.

### Models

* **DeepSeek-R1-Distill-Qwen-1.5B-Q4\_0.gguf**

  * Location: `C:\Users\yaswa\AppData\Local\nomic.ai\GPT4All\`
  * Format: `.gguf` (quantized for efficient CPU inference).
  * Backend: Loaded using `llama-cpp-python`.

### Frontend / App

* **[Streamlit](https://streamlit.io/)**

  * Used to build an interactive UI for querying the models.
  * Runs on Python 3.12 environment.
  * Provides easy debugging/logging via web interface.

---

## ðŸ”¹ Current Implementation

### Model Loading (working solution)

We are using `llama-cpp-python` to load DeepSeek locally:

```python
from llama_cpp import Llama

llm = Llama(
    model_path=r"C:\Users\yaswa\AppData\Local\nomic.ai\GPT4All\DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf",
    n_ctx=2048,
    n_threads=6,
    verbose=True
)
```

### Inference Function

```python
def ai1_response(prompt: str) -> str:
    try:
        output = llm(
            prompt,
            max_tokens=512,
            temperature=0.7,
            top_p=0.9,
            stop=["</s>"]
        )
        return output["choices"][0]["text"].strip()
    except Exception as e:
        return f"Error during generation: {e}"
```

## ðŸ”¹ Challenges Faced

1. **Tokenizer Issue with GPT4All**

   * Tried running `DeepSeek-R1-Distill-Qwen` via `gpt4all` Python bindings.
   * Encountered:

     ```
     ValueError: Failed to load the tokenizer. 
     Please ensure the model supports GPT4All or use a compatible one.
     ```
   * Root cause: GPT4All supports only a subset of `.gguf` models (mainly LLaMA/Falcon variants). Qwen-based DeepSeek models are **not yet supported**.

2. **Access Violation / LLaMA Load Failure**

   * When attempting to use `llama-cpp-python` for AI1 with the Qwen-based model, encountered:

     ```
     OSError: exception: access violation reading 0x0000000000000000
     ```
   * Root cause: The modelâ€™s tokenizer or backend was incompatible with the CPU and system spec (AMD Ryzen 7 5800HS, 8 cores, 32-bit/64-bit config). Some `.gguf` models require AVX/AVX2 or GPU CUDA support.
   * Resolution: Switched AI1 to a **GPT4All-compatible model** instead of using llama-cpp backend for this Qwen model.

3. **Environment Conflicts**

   * Needed to ensure Streamlit, GPT4All, and llama-cpp-python were all installed in the **same Python 3.12 environment**.
   * Resolved by explicitly installing with:

     ```bash
     C:\Users\yaswa\AppData\Local\Programs\Python\Python312\python.exe -m pip install gpt4all llama-cpp-python

     ```
3. **Local model Download Limiation**
    Decision to Use Hugging Face Pipeline Instead of Local Model Download

    Initially considered downloading a local Hugging Face model for offline inference.

    Challenge: Local models require significant disk space and longer load times for CPU inference.

    Resolution: Opted for Hugging Face transformers pipeline with flan-t5-small for lightweight, CPU-friendly, and reliable structured text generation. This also avoids system-specific DLL or CUDA issues.

## ðŸ”¹ Lessons Learned

* GPT4All app â‰  Python GPT4All bindings â€” they are separate.
* Not all `.gguf` models are cross-compatible between GPT4All and llama-cpp.
* For **Qwen-based DeepSeek models**, `llama-cpp-python` is the correct backend.
* Maintaining a clear record of model paths and environments avoids dependency issues.

---

## ðŸ”¹ Next Steps

* Add support for **multiple selectable models** in the app (DeepSeek, Falcon, Mistral).
* Benchmark inference speed across GPT4All vs llama-cpp backends.
* Extend the Streamlit UI with **chat history** and **multi-model dropdown**.
* Document performance (tokens/sec) and memory usage.

---

## ðŸ“Œ Summary

At this stage, we have:

* Installed and configured **Streamlit**, **GPT4All**, and **llama-cpp-python**.
* Successfully run **DeepSeek-R1-Distill-Qwen** locally via llama-cpp.
* Documented **GPT4All tokenizer failure** as a challenge.
* Established a foundation for **offline AI experimentation** with quantized models.

